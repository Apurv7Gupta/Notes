{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6ea72a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take messy user input, refine, answer\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph import START, END\n",
    "from typing import TypedDict\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30104120",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    return_full_text=False,   \n",
    "    max_new_tokens=150,      # hard stop\n",
    "    temperature=0.2,         # kill rambling\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.2   # stops loops\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "362bb4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class llmState(TypedDict):\n",
    "    input: str\n",
    "    output: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4fb40d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInput(state: llmState) -> llmState:\n",
    "    UserInput =  state[\"input\"]\n",
    "    refined=  model.invoke(\"Rewrite the following into a clear, concise English question. \"\"Output ONLY the rewritten question.\\n\\n\"f\"{UserInput}\").content\n",
    "    state[\"input\"] = refined\n",
    "    state[\"output\"] = model.invoke(f\"Answer the question clearly and directly in 3â€“5 sentences.\\n\\nQuestion: {refined}\").content\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8971e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(llmState)\n",
    "\n",
    "graph.add_node(\"getInput\", getInput)\n",
    "\n",
    "\n",
    "graph.add_edge(START, \"getInput\")\n",
    "graph.add_edge(\"getInput\", END)\n",
    "\n",
    "workflow = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4ed1c8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What is prompt engineering?', 'output': 'Prompt engineering is the process of designing and refining text prompts to elicit specific, accurate, and relevant responses from artificial intelligence (AI) models, such as language generators or chatbots. It involves understanding the capabilities and limitations of AI models and crafting prompts that effectively communicate the desired output. By optimizing prompts, users can improve the quality, relevance, and coherence of AI-generated responses. This technique has become increasingly important as AI models become more widespread and are used in various applications, including content creation, customer service, and research.'}\n"
     ]
    }
   ],
   "source": [
    "query =  {\"input\": \"??mahn whhhat izz prompt engg??\"}\n",
    "\n",
    "print(workflow.invoke(query))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LCvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
